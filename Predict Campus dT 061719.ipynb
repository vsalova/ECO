{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Load Packages and Classes"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import os, sys\n",
        "lib_path = os.path.abspath(os.path.join('../..', 'src')) # relative path of the source code in Box Folder \n",
        "sys.path.append(lib_path)\n",
        "\n",
        "from PI_client2 import *\n",
        "\n",
        "# libraries to plot\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "%matplotlib inline\n",
        "style.use('fivethirtyeight')\n",
        "#plt.style.use('dark_background')\n",
        "\n",
        "pc = pi_client ()\n",
        "\n",
        "import os, sys\n",
        "sys.path\n",
        "print os.path.abspath(os.path.join('../..', 'src'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "nbpresent": {
          "id": "ce9ad25d-ee58-469c-8e7f-8015f2a6c3fd"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Data from the PI System (Central data infrastrucure)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "points , _ = pc.search_by_point(\"*aitit**\")\n",
        "points"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "nbpresent": {
          "id": "a0737449-ff92-4c26-9680-f4b743dfc8bb"
        },
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If points are in csv:\n",
        "#mycsv = pd.read_csv(\"/Users/decolvin/Downloads/02152018-GBSF-flatlines.csv\")\n",
        "#mycsv2 = mycsv[\"Name\"]\n",
        "#points = list(mycsv2.values.flatten())\n",
        "#points"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs common to all the series\n",
        "points_selected = ['chcpchw1afi201',\n",
        "                  'chcpchw1ati202 ',\n",
        "                  'chcpchw1ati201',\n",
        "                  'teschw1aji7800',\n",
        "                  'TI7000R_SCL',\n",
        "                  'TI7000S_SCL',\n",
        "                   'OUTSIDE_AIR_WETBULB_TEMP',\n",
        "                   'OUTSIDE_AIR_HUMIDITY',\n",
        "                  'aiTIT4045']\n",
        "\n",
        "#points_selected = list(np.random.choice(points_selected[0:len(points_selected)],1000,replace=False))\n",
        "\n",
        "start = \"2015-01-01\"\n",
        "end = \"*\"\n",
        "interval=\"1h\"\n",
        "\nprint len(points_selected)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "nbpresent": {
          "id": "48ff1462-e508-4c25-a8a0-36d96dd79d7b"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calculation = \"calculated\"\n",
        "sumType=\"Average\" \n",
        "#sumType = \"PercentGood\"\n",
        "\n",
        "calculated = pc.get_stream_by_point(point_names = points_selected,\n",
        "                            start=start,\n",
        "                            end=end,\n",
        "                            calculation=calculation,\n",
        "                            interval=interval,\n",
        "                            sumType=sumType,\n",
        "                            label=None,\n",
        "                            dataserver=\"s09KoOKByvc0-uxyvoTV1UfQVVRJTC1QSS1Q\",\n",
        "                            WebID_dic=None)\n",
        "print calculated.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "nbpresent": {
          "id": "a2520a14-4ae5-433f-aba0-58fb404cc3c4"
        },
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calculated.tail()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot Data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import style\n",
        "style.use('fivethirtyeight')\n",
        "\n",
        "calculated.iloc[:,:].plot(figsize=(18,5), title=\"Data\",\n",
        "                          linewidth=2, colormap='viridis', legend=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "calculated.columns"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "q_chcp = calculated.iloc[:,0]*(calculated.iloc[:,1] - calculated.iloc[:,2])\n",
        "dt_chcp = calculated.iloc[:,1] - calculated.iloc[:,2]\n",
        "q_tes = calculated.teschw1aji7800\n",
        "dt_tes = calculated.TI7000R_SCL - calculated.TI7000S_SCL\n",
        "dt = (q_chcp*dt_chcp + q_tes*dt_tes)/(q_chcp+q_tes)\n",
        "\n",
        "calculated2 = pd.concat([calculated, q_chcp, dt_chcp, q_tes, dt_tes, dt], axis=1)\n",
        "calculated2.columns = list(calculated.columns) + [\"q_chcp\",\"dt_chcp\",\"q_tes\",\"dt_tes\",\"dt\"]\n",
        "calculated2.tail()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "calculated2.dt.plot(figsize=(18,5), linewidth=1, colormap='winter')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "calculated2['repump'] = 0\n",
        "calculated2.loc[\"2016-11-28\":\"2017-03-10\",'repump'] = 1\n",
        "calculated2.loc[\"2017-11-06\":\"2018-04-19\",'repump'] = 1 \n",
        "calculated2.repump = calculated2.repump.astype(\"category\")\n",
        "\ncalculated2['total tons'] = calculated2.q_chcp + calculated2.q_tes"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "delta = calculated2.loc[:,[\"aiTIT4045\",\"OUTSIDE_AIR_WETBULB_TEMP\",\"OUTSIDE_AIR_HUMIDITY\",\"repump\",\"total tons\",\"dt\"]]\n",
        "delta.rename(columns={\"aiTIT4045\":\"OAT\",\"dt\":\"deltat\"}, inplace=True)\n",
        "delta.tail()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print delta.shape\n",
        "print\n",
        "print delta.dtypes\n",
        "print\n",
        "print \"Number of missing values\"\n",
        "print delta.isnull().sum()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "### Remove rows where delta has missing values\n",
        "l1 = delta.loc[delta.deltat.isnull(),:]\n",
        "delta2 = delta.drop(l1.index)\n",
        "delta2.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "### Remove rows where OAT has missing values\n",
        "l2 = delta2.loc[delta2.OAT.isnull(),:]\n",
        "delta2 = delta2.drop(l2.index)\n",
        "delta2.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print \"Number of missing values\"\n",
        "print delta2.isnull().sum()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "### Add variables \n",
        "\n",
        "delta2[\"cdd\"] = delta2.OAT - 65.0\n",
        "delta2.loc[delta2.cdd < 0, \"cdd\"] = 0\n",
        "delta2[\"hdd\"] = 65.0 - delta2.OAT\n",
        "delta2.loc[delta2.hdd < 0, \"hdd\"] = 0\n",
        "\n",
        "delta2[\"cdd2\"] = delta2.cdd**2\n",
        "delta2[\"hdd2\"] = delta2.hdd**2\n",
        "\n",
        "delta2[\"OAT2\"] = delta2.OAT**2\n",
        "\n",
        "delta2[\"Date\"] = delta2.index.date\n",
        "filt = [\"delta\", \"cdd\", \"hdd\"]\n",
        "\n",
        "delta3 = delta2\n",
        "\n",
        "delta3[\"MONTH\"]= delta3.index.month\n",
        "delta3[\"MONTH\"] = delta3[\"MONTH\"].astype('category')\n",
        "delta3[\"TOD\"] = delta3.index.hour\n",
        "delta3[\"TOD\"] = delta3[\"TOD\"].astype('category')\n",
        "delta3[\"DOW\"]=delta3.index.weekday\n",
        "delta3[\"DOW\"] = delta3[\"DOW\"].astype('category')\n",
        "\n",
        "### Create dummy variables\n",
        "l3 = [\"MONTH\",\"TOD\",\"DOW\",\"repump\"]#,\"WEEK\"]#,\"DOY\"]\n",
        "delta3 = pd.get_dummies(data=delta3, columns=l3, drop_first=True)\n",
        "\n",
        "### Create Weekend flag\n",
        "WEEKEND= [0] * len(delta3.DOW_5)\n",
        "for i in range(0,len(delta3.DOW_5)):\n",
        "    if ((delta3.DOW_5.iloc[i] == 1) | (delta3.DOW_6.iloc[i] == 1)): \n",
        "        WEEKEND[i] = 1 \n",
        "else: 0\n",
        "\ndelta3[\"WEEKEND\"] = pd.Series(WEEKEND).values"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Lag Variables\n",
        "delta3['diff1'] = delta3.deltat.shift(1)\n",
        "delta3['diff2'] = delta3.deltat.shift(2)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "delta3.tail(1)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "### Summary\n",
        "print delta3.dtypes\n",
        "print\n",
        "print delta3.describe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data exploration for outliers"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot DeltaT data\n",
        "plt.figure()\n",
        "delta3[\"deltat\"].plot(figsize=(18,2), title=\"Data\",\n",
        "                          linewidth=2, color='green' ,legend=True)\n",
        "plt.figure()\n",
        "delta3.deltat.plot(kind='box',figsize=(18,2),vert=False)\n",
        "\n",
        "plt.figure()\n",
        "delta3.deltat.plot(kind='hist',figsize=(18,2), color='green')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "### Remove values less than 0\n",
        "print delta3.deltat.quantile([0.01,0.05,0.95,0.99])\n",
        "delta3.loc[delta3.deltat<0,\"deltat\"] = 0\n",
        "#delta3.delta.describe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "### Identify potential outliers\n",
        "q25 = delta3.deltat.quantile(0.25)\n",
        "q75 = delta3.deltat.quantile(0.75)\n",
        "iqr = q75 - q25\n",
        "outlier_above = q75 + iqr * 3\n",
        "outlier_below = q25 - iqr * 1.5\n",
        "print \"Q75 = %.0f\" %q75\n",
        "print \"Above threshold = %.0f\" %outlier_above\n",
        "print \"Below threshold = %.0f\" %outlier_below\n",
        "\n\n",
        "delta3.deltat.plot(style=\".\", figsize=(18,5), c='k', alpha=0.2, title=\"Potential Outliers in Red\")\n",
        "try:\n",
        "    delta3.loc[delta3.deltat >= outlier_above,'deltat'].plot(style=\".\", c='r')\n",
        "except:\n",
        "    pass\n",
        "#delta3.loc[delta3.delta <= outlier_below,'delta'].plot(style=\".\", c='r')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "### Remove outliers\n",
        "delta3 = delta3.loc[delta3.deltat < outlier_above, :]\n",
        "\n",
        "### Correlation\n",
        "delta3.plot.scatter(x='cdd', y='deltat',figsize=(18,2), color='blue')\n",
        "delta3.plot.scatter(x='hdd', y='deltat',figsize=(18,2), color='orange')\n",
        "delta3.plot.scatter(x='OAT', y='deltat',figsize=(18,2), color='green')\n",
        "print \"delta vs CDD = %.2f\" %delta3.deltat.corr(delta3.cdd)\n",
        "print \"delta vs HDD = %.2f\" %delta3.deltat.corr(delta3.hdd)\n",
        "print \"delta vs OAT = %.2f\" %delta3.deltat.corr(delta3.OAT)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Add more variables\n",
        "delta3['rolling7'] = delta3.deltat.rolling(3).mean()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "delta3.tail(1)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "delta3.columns"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x = ['OAT', 'rolling7']\n",
        "add = ['TOD_1', 'TOD_2',\n",
        "       'TOD_3', 'TOD_4', 'TOD_5', 'TOD_6', 'TOD_7', 'TOD_8', 'TOD_9',\n",
        "       'TOD_10', 'TOD_11', 'TOD_12', 'TOD_13', 'TOD_14', 'TOD_15',\n",
        "       'TOD_16', 'TOD_17', 'TOD_18', 'TOD_19', 'TOD_20', 'TOD_21',\n",
        "       'TOD_22', 'TOD_23', 'WEEKEND']\n",
        "\n",
        "y = delta3.deltat\n",
        "X = delta3[x]\n",
        "\n",
        "#split with time\n",
        "index_list = delta3.loc[(delta3.index < \"2018-07-01\") & (delta3.repump_1 == 1),:].index\n",
        "delta4 = delta3.drop(delta3.loc[index_list].index)\n",
        "y2= delta4.deltat\n",
        "\n",
        "X_train, X_test = delta4.loc[\"2017-01-01\":\"2018-08-01\",x], delta3.loc[\"2016-01-01\":\"2016-11-01\",x]\n",
        "y_train, y_test = y2.loc[\"2017-01-01\":\"2018-08-01\"], y.loc[\"2016-01-01\":\"2016-11-01\"]\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_train_sc, X_test_sc = pd.DataFrame(scaler.fit_transform(X_train)), pd.DataFrame(scaler.transform(X_test))\n",
        "X_train_sc = pd.concat([X_train_sc, delta4.loc[\"2017-01-01\":\"2018-08-01\",add].reset_index()], axis=1)\n",
        "X_test_sc = pd.concat([X_test_sc, delta4.loc[\"2016-01-01\":\"2016-11-01\",add].reset_index()], axis=1)\n",
        "\n",
        "X_train_sc.rename({0:'diff1',1:'OAT'}, axis=1, inplace=True)\n",
        "\n",
        "X_train_sc.index= delta4.loc[\"2017-01-01\":\"2018-08-01\",:].index\n",
        "X_test_sc.index = delta4.loc[\"2016-01-01\":\"2016-11-01\",:].index\n",
        "\n",
        "X_train_sc.drop('index', axis=1, inplace=True)\n",
        "X_test_sc.drop('index', axis=1, inplace=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def adj_r2_score(model,y,yhat):\n",
        "        \"\"\"Adjusted R square â€” put fitted linear model, y value, estimated y value in order\n",
        "        \n",
        "            Example:\n",
        "            In [142]: metrics.r2_score(diabetes_y_train,yhat)\n",
        "            Out[142]: 0.51222621477934993\n",
        "        \n",
        "            In [144]: adj_r2_score(lm,diabetes_y_train,yhat)\n",
        "            Out[144]: 0.50035823946984515\"\"\"\n",
        "        from sklearn import metrics\n",
        "        R2 = r2_score(y,yhat)\n",
        "        n = len(y)\n",
        "        p = len(model.coef_)\n",
        "        adj = 1-(1-R2)*(n-1)/(n-p-1)\n",
        "        return adj"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prophet"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from fbprophet import Prophet\n",
        "\n",
        "df = pd.DataFrame(delta3.deltat.resample(\"1D\").mean().reset_index().copy())\n",
        "df.rename({\"index\":\"ds\",\"deltat\":\"y\"}, axis=1, inplace=True)\n",
        "df.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "m = Prophet()\n",
        "m.fit(df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "future = m.make_future_dataframe(freq='D',periods=(30))\n",
        "future.tail()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "forecast = m.predict(future)\n",
        "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
        "#forecast.tail()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fig1 = m.plot(forecast)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fig2 = m.plot_components(forecast)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "### Statsmodels\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "#X_train1 = sm.add_constant(X_train)\n",
        "sm = smf.OLS(y_train, X_train_sc)\n",
        "sm_fit = sm.fit()\n",
        "#print sm_fit.summary() # 0.986\n",
        "#print\n",
        "#print r2_score(y_train,sm_fit.predict(X_train)) # 0.959\n",
        "sm_rmse_train = math.sqrt(mean_squared_error(y_train, sm_fit.predict(X_train_sc)))\n",
        "print \"CVRMSE = %.2f\" %((math.sqrt(sum((y_train-sm_fit.predict(X_train_sc))**2)/(len(y_train)-len(sm_fit.params)))) / y_train.mean())\n",
        "#print \"Train RMSE = \", sm_rmse_train\n",
        "\n",
        "sm_pred = sm_fit.predict(X_test_sc)\n",
        "#print \"R2 Test Set = \", r2_score(y_test, sm_pred) # 0.966\n",
        "sm_rmse = math.sqrt(mean_squared_error(y_test, sm_pred))\n",
        "#print \"RMSE = \", sm_rmse\n",
        "print \n",
        "\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Features\"] = X_train.columns\n",
        "vif[\"VIF Factor\"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n",
        "print vif[vif[\"VIF Factor\"] >10]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "### Scikit Learn\n",
        "from sklearn import linear_model\n",
        "lm = linear_model.LinearRegression()\n",
        "lm_fit = lm.fit(y=y_train,X=X_train)\n",
        "print \"Linear Regression Train Adj R2 = %.2f\" %adj_r2_score(lm_fit,y_train,lm.predict(X_train))\n",
        "print\n",
        "\n",
        "lm_pred = lm_fit.predict(X_test)\n",
        "print \"Linear Regression Test Adj R2 = %.2f\" %adj_r2_score(lm_fit,y_test,lm.predict(X_test))\n",
        "print\n",
        "print \"CVRMSE = %.2f\" %(math.sqrt(mean_squared_error(y_train,lm_fit.predict(X_train)))/ y_train.mean())\n",
        "lm_rmse = math.sqrt(mean_squared_error(y_test, lm_pred))\n",
        "print\n",
        "print \"RMSE on Test set = %.2f\" %(math.sqrt(mean_squared_error(y_test,lm_fit.predict(X_test))))\n",
        "\n",
        "#y_test.plot()\n",
        "#mod_lm_pred.plot()\n",
        "\n",
        "lm_df = pd.DataFrame({\"Actual\":y_test,\"Pred\":lm_pred})\n",
        "#lm_df = pd.DataFrame({\"Actual\":y_train,\"Pred\":lm.predict(X_train)})\n",
        "lm_df.plot(figsize=(18,2), linewidth=2, colormap='winter', legend=True, title=\"Linear Regression over Test Period\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(18,2))\n",
        "ax.scatter(lm_df.Actual, lm_df.Pred, alpha=0.2)\n",
        "ax.set_xlabel('Measured')\n",
        "ax.set_ylabel('Predicted')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tree-Based Methods"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### H2O"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import h2o\n",
        "h2o.cluster().shutdown()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import h2o\n",
        "\n",
        "h2o.init()\n",
        "\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "df = h2o.H2OFrame(delta3)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = \"deltat\"\n",
        "#predictors = df.columns.remove(response)\n",
        "rm_list = ['Date','diff1','diff2','OAT_shift1','OAT_shift2','deltat']\n",
        "predictors = [x for x in df.columns if x not in rm_list]\n",
        "\n",
        "train, valid, test = df.split_frame(\n",
        "    ratios=[0.8,0.1],\n",
        "    seed=7,\n",
        "    destination_frames=['train.hex','valid.hex','test.hex'])\n",
        "\n",
        "train_h2o = h2o.H2OFrame(delta4[\"2017-01-01\":\"2018-08-01\"])\n",
        "test_h2o = h2o.H2OFrame(delta3[\"2016-01-01\":\"2016-11-01\"])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from h2o.grid.grid_search import H2OGridSearch\n",
        "\n",
        "# Exhaustive Grid Search\n",
        "# GBM hyperparameters\n",
        "gbm_params1 = {'learn_rate': [0.01, 0.1],\n",
        "               'max_depth': [3, 5, 9]}\n",
        "               #'sample_rate': [0.8, 1.0],\n",
        "               #'col_sample_rate': [0.2, 0.5, 1.0]}\n",
        "\n",
        "# Train and validate a cartesian grid of GBMs\n",
        "gbm_grid1 = H2OGridSearch(model=H2OGradientBoostingEstimator,\n",
        "                          grid_id='gbm_grid1',\n",
        "                          hyper_params=gbm_params1)\n",
        "gbm_grid1.train(x=predictors, y=response,\n",
        "                training_frame=train_h2o,\n",
        "                ntrees=200,\n",
        "                seed=7,\n",
        "                validation_frame=test_h2o)#,\n",
        "               #nfolds=5, fold_assignment = 'AUTO', keep_cross_validation_predictions=True)\n",
        "\n",
        "# Get the grid results, sorted by validation mse\n",
        "gbm_gridperf1 = gbm_grid1.get_grid(sort_by='mse', decreasing=False)\n",
        "\n",
        "# Grab the top GBM model, chosen by validation mse\n",
        "best_gbm1 = gbm_gridperf1.models[0]\n",
        "\n",
        "# Now let's evaluate the model performance on a test set\n",
        "# so we get an honest estimate of top model performance\n",
        "best_gbm_perf1 = best_gbm1.model_performance(test)\n",
        "best_gbm_perf1"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "sh = best_gbm1.score_history()\n",
        "sh = pd.DataFrame(sh)\n",
        "sh.plot(x='number_of_trees', y = ['training_rmse','validation_rmse'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "preds = best_gbm1.predict(h2o.H2OFrame(delta4))\n",
        "\n",
        "com = pd.DataFrame({\"Actual\":delta4.deltat.values, \"GBM\":preds.as_data_frame().values.reshape(delta4.deltat.values.shape[0],)})\n",
        "com.plot(figsize=(18,5), linewidth=1, colormap='viridis')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "h2o.cluster().shutdown()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LightGBM"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "estimator = lgb.LGBMRegressor(random_state=7)\n",
        "\n",
        "param_grid = {'learning_rate': [0.01, 0.1],\n",
        "               'max_depth': [3, 5, 9],\n",
        "              'min_child_weight': [1,3,5,7]}\n",
        "\n",
        "gbm2 = GridSearchCV(estimator, param_grid)\n",
        "\n",
        "gbm2.fit(delta4.loc[\"2017-01-01\":\"2018-01-01\",predictors],\n",
        "                                     delta4.loc[\"2017-01-01\":\"2018-01-01\",'deltat'])\n",
        "\n",
        "print(\"RMSE on Test set = %.3f\" %(mean_squared_error(delta3.loc[\"2016-01-01\":\"2017-01-01\",'deltat'],\n",
        "                                                     gbm2.best_estimator_.predict(delta3.loc[\"2016-01-01\":\"2017-01-01\",predictors])) ** 0.5))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "gbm2_df = pd.DataFrame({\"Actual\":delta3.loc[\"2016-01-01\":\"2017-01-01\",'deltat'],\n",
        "                        \"GBM2\":gbm2.best_estimator_.predict(delta3.loc[\"2016-01-01\":\"2017-01-01\",predictors])},\n",
        "                       index=delta3.loc[\"2016-01-01\":\"2017-01-01\",'deltat'].index)\n",
        "\ngbm2_df.plot(figsize=(18,5), linewidth=1, colormap='viridis')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGBoost"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "# GBM hyperparameters\n",
        "xgb_params = {'learning_rate': [0.01, 0.1],\n",
        "               'max_depth': [3, 5, 9],\n",
        "              'min_child_weight': [1,3,5,7]}\n",
        "\n",
        "xgb_mod = XGBRegressor(random_state=7, n_jobs=-1)\n",
        "\n",
        "gs_xgb = GridSearchCV(xgb_mod,\n",
        "                      param_grid=xgb_params,\n",
        "                      cv=5,\n",
        "                      verbose=2).fit(delta4.loc[\"2017-01-01\":\"2018-01-01\",predictors],\n",
        "                                     delta4.loc[\"2017-01-01\":\"2018-01-01\",'deltat'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(gs_xgb.best_params_)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ypred = gs_xgb.predict(delta3.loc[\"2016-01-01\":\"2017-01-01\",predictors])\n",
        "print(\"RMSE over Test period = %.3f\" %(mean_squared_error(delta3.loc[\"2016-01-01\":\"2017-01-01\",'deltat'], ypred)**0.5))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_df = pd.DataFrame({\"Actual\":delta3.loc[\"2016-01-01\":\"2017-01-01\",'deltat'],\"XGB\":ypred},\n",
        "                      index=delta3[\"2016-01-01\":\"2017-01-01\"].index)\n",
        "\nxgb_df.plot(figsize=(18,5), linewidth=1, colormap='viridis')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest, AdaBoost, Gradient Boostig (SciKit Learn)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "## Warning: this cell takes ~5 minutes to run\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "#from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "## Scale data\n",
        "#scaler = StandardScaler()\n",
        "#X_train_std = scaler.fit_transform(X_train)\n",
        "#X_test_std = scaler.transform(X_test)\n",
        "\n",
        "## Grid Search Decision Trees\n",
        "params_dt = {'max_depth':[1,2,3,4,5,6],'min_samples_split':[2,3,4,5,6]}\n",
        "dt = DecisionTreeRegressor(random_state=1)\n",
        "gs_dt = GridSearchCV(dt,param_grid=params_dt).fit(X_train,y_train)\n",
        "#scores_dt = cross_val_score(gs_dt, X_train, y_train)\n",
        "#print scores_dt.mean()                             \n",
        "\n",
        "## Grid Search Random Forest\n",
        "params_rf = {'max_depth':[1,2,3,4,5,6],'min_samples_split':[2,3,4,5,6],'n_estimators':[1,10,50,100]}\n",
        "rf = RandomForestRegressor(random_state=1)\n",
        "gs_rf = GridSearchCV(rf,param_grid=params_rf).fit(X_train,y_train)\n",
        "#scores_rt = cross_val_score(gs_rf, X_train, y_train)\n",
        "#print scores_rt.mean()                             \n",
        "\n",
        "## Grid Search Gradient Boost\n",
        "params_boost = {'max_depth':[1,2,3,4,5,6],'learning_rate':[0.01,0.1,1,10], 'n_estimators':[1,10,50,100]}                                                  \n",
        "boost = GradientBoostingRegressor(random_state=1)\n",
        "gs_boost = GridSearchCV(boost,param_grid=params_boost).fit(X_train,y_train)\n",
        "#scores_boost = cross_val_score(gs_boost, X_train, y_train)\n",
        "#print scores_boost.mean()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print \"Decision Tree R2 = %.2f\" %r2_score(y_train, gs_dt.best_estimator_.predict(X_train))\n",
        "print \"Random Forest R2 = %.2f\" %r2_score(y_train, gs_rf.best_estimator_.predict(X_train))\n",
        "#print \"Adaboost R2 = %.2f\" %r2_score(y_train, gs_adaboost.best_estimator_.predict(X_train))\n",
        "print \"Gradient Boost R2 = %.2f\" %r2_score(y_train, gs_boost.best_estimator_.predict(X_train))\n",
        "print\n",
        "print \"Decision Tree Test R2 = %.2f\" %r2_score(y_test, gs_dt.best_estimator_.predict(X_test))\n",
        "print \"Random Forest Test R2 = %.2f\" %r2_score(y_test, gs_rf.best_estimator_.predict(X_test))\n",
        "#print \"Adaboost Test R2 = %.2f\" %r2_score(y_test, gs_adaboost.best_estimator_.predict(X_test))\n",
        "print \"Gradient Boost Test R2 = %.2f\" %r2_score(y_test, gs_boost.best_estimator_.predict(X_test))\n",
        "print\n",
        "#print \"Decision Tree CVRMSE = %.2f\" %((math.sqrt(sum((y_train-gs_dt.predict(X_train))**2)/(len(y_train)-len(lm.coef_)))) / y_train.mean())\n",
        "#print \"Random Forest CVRMSE = %.2f\" %((math.sqrt(sum((y_train-gs_rf.predict(X_train))**2)/(len(y_train)-len(lm.coef_)))) / y_train.mean())\n",
        "#print \"Adaboost CVRMSE = %.2f\" %((math.sqrt(sum((y_train-gs_adaboost.predict(X_train))**2)/(len(y_train)-len(lm.coef_)))) / y_train.mean())\n",
        "#print \"Gradient Boost CVRMSE = %.2f\" %((math.sqrt(sum((y_train-gs_boost.predict(X_train))**2)/(len(y_train)-len(lm.coef_)))) / y_train.mean())\n",
        "\n",
        "print \"Decision Tree RMSE = %.2f\" %math.sqrt(mean_squared_error(y_test, gs_dt.best_estimator_.predict(X_test)))\n",
        "print \"Random Forest RMSE = %.2f\" %math.sqrt(mean_squared_error(y_test, gs_rf.best_estimator_.predict(X_test)))\n",
        "#print \"Adaboost RMSE = %.2f\" %math.sqrt(mean_squared_error(y_test, gs_adaboost.best_estimator_.predict(X_test)))\n",
        "print \"Gradient Boost RMSE = %.2f\" %math.sqrt(mean_squared_error(y_test, gs_boost.best_estimator_.predict(X_test)))\n",
        "\n",
        "# Plot the results\n",
        "boost_df = pd.DataFrame({\"Actual\":y_test,\"Pred\":gs_boost.predict(X_test)})\n",
        "\n",
        "#boost_df.plot(figsize=(18,2),linewidth=2, colormap='winter', legend=True, title=\"Gradient Boost over Test Period\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(18,2))\n",
        "ax.scatter(boost_df.Actual, boost_df.Pred, alpha=0.2)\n",
        "ax.set_xlabel('Measured')\n",
        "ax.set_ylabel('Predicted')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "gs_rf.best_estimator_"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recursive Feature Elimination"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.feature_selection import RFE, RFECV\n",
        "from sklearn.cross_validation import StratifiedKFold\n",
        "\n",
        "np.random.seed(7)\n",
        "model = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=6,\n",
        "           max_features='auto', max_leaf_nodes=None,\n",
        "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
        "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
        "           n_estimators=100, n_jobs=1, oob_score=False, random_state=1,\n",
        "           verbose=0, warm_start=False)\n",
        "rfe = RFE(model)\n",
        "rfe_fit = rfe.fit(X_train_sc, y_train)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_sc = X_train_sc[X_train_sc.columns[rfe_fit.support_]]\n",
        "X_train_sc.head()\n",
        "\nX_test_sc = X_test_sc[X_test_sc.columns[rfe_fit.support_]]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Regression"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "## Scale data\n",
        "scaler = MinMaxScaler()\n",
        "X_train_std = scaler.fit_transform(X_train)\n",
        "X_test_std = scaler.transform(X_test)\n",
        "\n",
        "svr = svm.SVR(kernel='rbf', shrinking=False, verbose=2)\n",
        "\n",
        "#params_svr = {'kernel':['linear','poly','rbf','sigmoid'],'epsilon':[0.01,0.1,1,10], 'C':[1,50,100]}\n",
        "params_svr = {'epsilon':[0.01,0.1,1,10], 'C':[1,100,500,1000]}\n",
        "gs_svr = GridSearchCV(svr,param_grid=params_svr).fit(X_train_std,y_train)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print gs_svr.best_estimator_\n",
        "print ''\n",
        "\n",
        "print \"SVM Train R2 = %.2f\" %r2_score(y_train, gs_svr.best_estimator_.predict(X_train_std))\n",
        "print \"SVM Test R2 = %.2f\" %r2_score(y_test, gs_svr.best_estimator_.predict(X_test_std))\n",
        "print \"SVM RMSE = %.3f\" %math.sqrt(mean_squared_error(y_test, gs_svr.best_estimator_.predict(X_test_std)))\n",
        "\n",
        "svr_df = pd.DataFrame({\"Actual\":y_test,\"Pred\":gs_svr.best_estimator_.predict(X_test_std)})\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(18,2))\n",
        "ax.scatter(svr_df.Actual, svr_df.Pred, alpha=0.2)\n",
        "ax.set_xlabel('Measured')\n",
        "ax.set_ylabel('Predicted')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Artificial Neural Network"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = ['OAT', 'TOD_1', 'TOD_2',\n",
        "       'TOD_3', 'TOD_4', 'TOD_5', 'TOD_6', 'TOD_7', 'TOD_8', 'TOD_9',\n",
        "       'TOD_10', 'TOD_11', 'TOD_12', 'TOD_13', 'TOD_14', 'TOD_15',\n",
        "       'TOD_16', 'TOD_17', 'TOD_18', 'TOD_19', 'TOD_20', 'TOD_21',\n",
        "       'TOD_22', 'TOD_23', 'WEEKEND', 'rolling7']\n",
        "\n",
        "#X2_train, X2_test = delta3.loc[\"2017-07-01\":\"2018-07-01\",x2], delta3.loc[\"2016-01-01\":\"2017-07-01\",x2]\n",
        "#y_train, y_test = y.loc[\"2017-07-01\":\"2018-07-01\"], y.loc[\"2016-01-01\":\"2017-07-01\"]\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "## Scale data\n",
        "scaler = MinMaxScaler()\n",
        "X_train_std = scaler.fit_transform(X_train)\n",
        "X_test_std = scaler.transform(X_test)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "ann = MLPRegressor(random_state=7, max_iter=500, learning_rate='adaptive', early_stopping=True, verbose=True)\n",
        "\n",
        "params_ann = {'batch_size':[10,20,50],'hidden_layer_sizes':[1,10,100,500,(100,100)],'learning_rate_init':[0.001,0.01,0.1]}\n",
        "gs_ann = GridSearchCV(ann,param_grid=params_ann,cv=5).fit(X_train_sc,y_train)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gs_ann.best_estimator_"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "#ann = MLPRegressor(random_state=1, max_iter=1000)\n",
        "ann = MLPRegressor(random_state=7, max_iter=500, learning_rate='adaptive', hidden_layer_sizes=100,\n",
        "                  learning_rate_init=0.001, solver='adam', early_stopping=True)\n",
        "\n",
        "#ann_mod = ann.fit(X_train_std,y_train)\n",
        "ann_mod = gs_ann.best_estimator_.fit(X_train_sc, y_train)\n",
        "ann_mod"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print \"ANN Train R2 = %.2f\" %r2_score(y_train, ann_mod.predict(X_train_sc))\n",
        "print \"ANN Test R2 = %.2f\" %r2_score(y_test, ann_mod.predict(X_test_sc))\n",
        "print \"ANN RMSE = %.2f\" %math.sqrt(mean_squared_error(y_test, ann_mod.predict(X_test_sc)))\n",
        "\n",
        "print \"CVRMSE = %.2f\" %(math.sqrt(mean_squared_error(y_train,ann_mod.predict(X_train_sc)))/ y_train.mean())\n",
        "\n",
        "ann_df = pd.DataFrame({\"Actual\":y_test,\"Pred\":ann_mod.predict(X_test_sc)})\n",
        "ann_df[0:50].plot(figsize=(18,2), linewidth=2, colormap='winter', legend=True, title=\"ANN over Test Period\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(18,2))\n",
        "ax.scatter(ann_df.Actual, ann_df.Pred, alpha=0.2)\n",
        "ax.set_xlabel('Measured')\n",
        "ax.set_ylabel('Predicted')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare model RMSE"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print \"Linear RMSE = %.3f\" %lm_rmse\n",
        "print \"XGB RMSE = %.3f\" %(math.sqrt(mean_squared_error(delta3.loc[\"2016-01-01\":\"2017-01-01\",'deltat'],\n",
        "                                                       gs_xgb.best_estimator_.predict(delta3.loc[\"2016-01-01\":\"2017-01-01\",predictors]))))\n",
        "#print \"Random Forest RMSE = %.3f\" %(math.sqrt(mean_squared_error(y_test, gs_rf.best_estimator_.predict(X_test))))\n",
        "#print \"Boosting RMSE = %.3f\" %(math.sqrt(mean_squared_error(y_test, gs_boost.best_estimator_.predict(X_test))))\n",
        "print \"Artificial Neural Network RMSE = %.3f\" %(math.sqrt(mean_squared_error(y_test, ann_mod.predict(X_test_sc))))\n",
        "print \"Support Vector Regression RMSE = %.3f\" %(math.sqrt(mean_squared_error(y_test, gs_svr.best_estimator_.predict(X_test_std))))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keras NN"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.optimizers import *\n",
        "from keras.layers import *\n",
        "import keras.backend as K\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "np.random.seed(7)\n",
        "\ncallbacks_list = [EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001, verbose=2)]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Set training and test sets"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler(feature_range=(0,1))\n",
        "\n",
        "#x = ['OAT','TOD_1', 'TOD_2',\n",
        "#       'TOD_3', 'TOD_4', 'TOD_5', 'TOD_6', 'TOD_7', 'TOD_8', 'TOD_9',\n",
        "#       'TOD_10', 'TOD_11', 'TOD_12', 'TOD_13', 'TOD_14', 'TOD_15',\n",
        "#       'TOD_16', 'TOD_17', 'TOD_18', 'TOD_19', 'TOD_20', 'TOD_21',\n",
        "#       'TOD_22', 'TOD_23', 'WEEKEND', 'diff1']\n",
        "\n",
        "x = ['OAT', 'rolling7']\n",
        "\n",
        "X_train, X_test = delta4.loc[\"2017-07-01\":\"2018-07-01\",delta4.columns == 'deltat'], delta4.loc[\"2016-01-01\":\"2016-11-01\",delta4.columns == 'deltat']\n",
        "X_train, X_test = delta4.loc[\"2017-07-01\":\"2018-07-01\",x], delta4.loc[\"2016-01-01\":\"2016-11-01\",x]\n",
        "\n",
        "X_train_sc = sc.fit_transform(X_train)\n",
        "X_test_sc = sc.transform(X_test)\n",
        "\n",
        "#for s in range(1,2):\n",
        "#    X_train['X_{}'.format(s)] = X_train['deltat'].shift(s)\n",
        "#    X_test['X_{}'.format(s)] = X_test['deltat'].shift(s)\n",
        "\n",
        "#X_train_sc = X_train.dropna().drop('deltat', axis=1)\n",
        "#y_train_sc = X_train.dropna().drop('X_1', axis=1)\n",
        "y_train_sc = delta4.loc[\"2017-07-01\":\"2018-07-01\",'deltat']\n",
        "\n",
        "#X_test_sc = X_test.dropna().drop('deltat', axis=1)\n",
        "#y_test_sc = X_test.dropna().drop('X_1', axis=1)\n",
        "y_test_sc = delta4.loc[\"2016-01-01\":\"2016-11-01\",'deltat']\n",
        "\n",
        "X_train_sc = X_train_sc\n",
        "y_train_sc = y_train_sc\n",
        "\n",
        "X_test_sc = X_test_sc\n",
        "y_test_sc = y_test_sc"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train size: (%d x %d)'%(X_train_sc.shape[0], X_train_sc.shape[1]))\n",
        "print('Test size: (%d x %d)'%(X_test_sc.shape[0], X_test_sc.shape[1]))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape input to be [samples, time steps, features]\n",
        "#X_train_sc = np.reshape(X_train_sc, (X_train_sc.shape[0], 1, X_train_sc.shape[1]))\n",
        "#X_test_sc = np.reshape(X_test_sc, (X_test_sc.shape[0], 1, X_test_sc.shape[1]))\n",
        "#y_train_sc = np.reshape(y_train_sc, (y_train_sc.shape[0], 1, 1))\n",
        "#y_test_sc = np.reshape(y_test_sc, (y_test_sc.shape[0], 1, 1))\n",
        "\n",
        "#print('Train size: (%d x %d x %d)' %(X_train_sc.shape[0], X_train_sc.shape[1], X_train_sc.shape[2]))\n",
        "#print('Test size: (%d x %d x %d)' %(X_test_sc.shape[0], X_test_sc.shape[1], X_test_sc.shape[2]))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define models"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create model, required for KerasRegressor\n",
        "def create_model1():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(20, input_shape=(X_train_sc.shape[1],), activation='relu', kernel_initializer='lecun_uniform'))\n",
        "    model.add(Dense(1))\n",
        "    # Compile model\n",
        "    model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
        "    return model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape to 3D for LSTM\n",
        "X_train_sc_3d = np.reshape(X_train_sc, (X_train_sc.shape[0], 1, X_train_sc.shape[1]))\n",
        "X_test_sc_3d = np.reshape(X_test_sc, (X_test_sc.shape[0], 1, X_test_sc.shape[1]))\n",
        "print X_train_sc_3d.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create model, required for KerasRegressor\n",
        "def create_model2():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(100,input_shape=(X_test_sc_3d.shape[1], X_test_sc_3d.shape[2]), return_sequences=True))\n",
        "    #model.add(Dropout(0.2))\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dense(1))\n",
        "    # Compile model\n",
        "    model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
        "    return model\n",
        "\"\"\" Here we are creating a model with 2 layers, both of LSTM type with 100 nodes each \"\"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Set model as 'model'"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# create model\n",
        "\n",
        "model1 = KerasRegressor(build_fn=create_model1, verbose=2, batch_size=16)\n",
        "\nmodel2 = KerasRegressor(build_fn=create_model2, verbose=2, batch_size=16)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid search (time intensive search)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 1"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "np.random.seed(7)\n",
        "\n",
        "# define the grid search parameters\n",
        "batch_size = [16, 32]\n",
        "epochs = [50, 100]\n",
        "#neurons = [1, 5, 10]\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "grid = GridSearchCV(estimator=model1, param_grid=param_grid, cv=3)\n",
        "\nkeras_grid = grid.fit(X_train_sc, y_train_sc)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 2"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "np.random.seed(7)\n",
        "\n",
        "# define the grid search parameters\n",
        "batch_size = [16, 32]\n",
        "epochs = [50, 100]\n",
        "#neurons = [1, 5, 10]\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "grid = GridSearchCV(estimator=model2, param_grid=param_grid, cv=3)\n",
        "\nkeras_grid2 = grid.fit(X_train_sc_3d, y_train_sc)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Summarize best model and fit it to 'history'"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize results - model1\n",
        "print(\"Best: %f using %s\" % (keras_grid.best_score_, keras_grid.best_params_))\n",
        "means = keras_grid.cv_results_['mean_test_score']\n",
        "stds = keras_grid.cv_results_['std_test_score']\n",
        "params = keras_grid.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "# summarize results - model2\n",
        "print(\"Best: %f using %s\" % (keras_grid2.best_score_, keras_grid2.best_params_))\n",
        "means = keras_grid2.cv_results_['mean_test_score']\n",
        "stds = keras_grid2.cv_results_['std_test_score']\n",
        "params = keras_grid2.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "#callbacks_list = [EarlyStopping(monitor='val_loss', patience=3, min_delta=0.001, verbose=0)]\n",
        "#np.random.seed(7)\n",
        "\n",
        "history = keras_grid.best_estimator_.fit(X_train_sc, y_train_sc, verbose=0)\n",
        "\n",
        "history2 = keras_grid2.best_estimator_.fit(X_train_sc_3d, y_train_sc, verbose=0)\n",
        "\n",
        "print('R-Squared Train: %f' %(r2_score(y_train_sc, history.model.predict(X_train_sc))))\n",
        "print('R-Squared Test: %f' %(r2_score(y_test_sc, history.model.predict(X_test_sc))))\n",
        "print(\"Keras RMSE = %.3f\" %(math.sqrt(mean_squared_error(y_test_sc, history.model.predict(X_test_sc)))))\n",
        "\n",
        "print \n",
        "\n",
        "print('R-Squared Train: %f' %(r2_score(y_train_sc, history2.model.predict(X_train_sc_3d))))\n",
        "print('R-Squared Test: %f' %(r2_score(y_test_sc, history2.model.predict(X_test_sc_3d))))\n",
        "print(\"Keras RMSE = %.3f\" %(math.sqrt(mean_squared_error(y_test_sc, history2.model.predict(X_test_sc_3d)))))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comp = pd.DataFrame({\"Actual\":y_test_sc,\n",
        "                     \"Model1\":np.reshape(history.model.predict(X_test_sc),history.model.predict(X_test_sc).shape[0]),\n",
        "                     \"Model2\":np.reshape(history2.model.predict(X_test_sc_3d),history2.model.predict(X_test_sc_3d).shape[0])})\n",
        "\n",
        "comp.iloc[3000:4000,:].plot(figsize=(18,5), legend=True, colormap='viridis', linewidth=1, title=\"Comparison on Test Dataset\")\n",
        "\n",
        "#plt.figure(figsize=(18,5))\n",
        "#plt.plot(y_test_sc, linewidth=1)\n",
        "#plt.plot(history.model.predict(X_test_sc), linewidth=1)\n",
        "#plt.plot(history2.model.predict(X_test_sc_3d), linewidth=1)\n",
        "#plt.title(\"Comparison on Test Dataset\")\n",
        "#plt.legend([\"Actual\",\"Model1\",\"Model2\"])\n",
        "#plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Future Forecast"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "forecast , _ = pc.search_by_point(\"*forecast*\")\n",
        "forecast"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "end = pd.Timestamp.now() + timedelta(days=6.5)\n",
        "\n",
        "# inputs common to all the series\n",
        "points_selected = forecast[0]\n",
        "\n",
        "#points_selected = list(np.random.choice(points_selected[0:4000],50,replace=False))\n",
        "\n",
        "start = pd.Timestamp.now()\n",
        "start = \"2018-08-03 11:00:00\"\n",
        "end = end\n",
        "end = \"2018-08-13\"\n",
        "interval=\"1h\"\n",
        "\nprint len(points_selected)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "calculation = \"calculated\"\n",
        "sumType=\"Average\" \n",
        "#sumType = \"PercentGood\"\n",
        "\n",
        "forecast = pc.get_stream_by_point(point_names = points_selected,\n",
        "                            start=start,\n",
        "                            end=end,\n",
        "                            calculation=calculation,\n",
        "                            interval=interval,\n",
        "                            sumType=sumType,\n",
        "                            label=None,\n",
        "                            dataserver=\"s09KoOKByvc0-uxyvoTV1UfQVVRJTC1QSS1Q\",\n",
        "                            WebID_dic=None)\n",
        "print forecast.shape\n",
        "\nforecast.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "forecast.rename({'Outside_Air_Temp_Forecast':'OAT'}, axis=1, inplace=True)\n",
        "forecast['rolling7'] = delta3.rolling7.rolling(3).mean()[-1]\n",
        "\nforecast.dropna(inplace=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "forecast.iloc[1,1] = np.mean([delta3.deltat[-2],delta3.deltat[-1],forecast.rolling7[0]])\n",
        "forecast.iloc[2,1] = np.mean([delta3.deltat[-1],forecast.rolling7[0],forecast.rolling7[1]])\n",
        "print forecast.shape\n",
        "for i in range(3,forecast.shape[0]):\n",
        "    forecast.iloc[i,1] = np.mean([forecast.rolling7[i-3],forecast.rolling7[i-1],forecast.rolling7[i-1]])\n",
        "print forecast.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_sc = scaler.transform(forecast)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "forecast_sc2 = pd.DataFrame(scaler.inverse_transform(forecast_sc), columns=[\"OAT\",\"diff1\"])\n",
        "for i in range(3):\n",
        "#for i in range(forecast_sc.shape[0]):\n",
        "    result.append(sm_fit.predict(forecast_sc2.iloc[i,:])[0])\n",
        "    #inv.append((result[i] * (X_train['diff1'].max(axis=0) - X_train['diff1'].min(axis=0))) + X_train['diff1'].min(axis=0))\n",
        "    #forecast_sc.iloc[i+1,1] = (result[i] - X_train['diff1'].min(axis=0)) / (X_train['diff1'].max(axis=0) - X_train['diff1'].min(axis=0))\n",
        "    forecast_sc2.iloc[i+1,1] = result[i]\n",
        "    forecast_sc = scaler.transform(forecast_sc2)\n",
        "    #forecast_sc.iloc[i+1,1] = scaler.transform(result[i])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_sc['result'] = result\n",
        "#forecast_sc['manual_predict'] = forecast_sc['OAT']*x1 + forecast_sc['diff1']*x2\n",
        "forecast_sc['diff1_inv'] = (forecast_sc['diff1']  * (X_train['diff1'].max(axis=0) - X_train['diff1'].min(axis=0))) + X_train['diff1'].min(axis=0)\n",
        "forecast_sc.head(20)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_sc.result.plot(figsize=(18,5), colormap='viridis', legend=True)\n",
        "forecast_sc.diff1_inv.plot(legend=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Add variables \n",
        "forecast.rename(columns={'Outside_Air_Temp_Forecast':'OAT'},inplace=True)\n",
        "forecast2 = forecast\n",
        "\n",
        "forecast2[\"cdd\"] = forecast.OAT - 65.0\n",
        "forecast2.loc[forecast2.cdd < 0, \"cdd\"] = 0\n",
        "\n",
        "forecast2[\"hdd\"] = 65.0 - forecast.OAT\n",
        "forecast2.loc[forecast2.hdd < 0, \"hdd\"] = 0\n",
        "\n",
        "forecast2[\"cdd2\"] = forecast2.cdd**2\n",
        "forecast2[\"hdd2\"] = forecast2.hdd**2\n",
        "\n",
        "#forecast2[\"YEAR\"]=forecast22.index.year\n",
        "forecast2[\"MONTH\"]= forecast2.index.month\n",
        "forecast2[\"MONTH\"] = forecast2[\"MONTH\"].astype('category')\n",
        "forecast2[\"TOD\"] = forecast2.index.hour\n",
        "forecast2[\"TOD\"] = forecast2[\"TOD\"].astype('category')\n",
        "forecast2[\"DOW\"] = forecast2.index.weekday\n",
        "forecast2[\"DOW\"] = forecast2[\"DOW\"].astype('category')\n",
        "#forecast2[\"WEEK\"] = forecast2.index.week\n",
        "#forecast2[\"WEEK\"] = forecast2[\"WEEK\"].astype('category')\n",
        "#forecast2[\"DOY\"]=forecast2.index.dayofyear\n",
        "#forecast2[\"DOY\"] = forecast2[\"DOY\"].astype('category')\n",
        "\n",
        "### Create dummy variables\n",
        "l3 = [\"MONTH\",\"TOD\",\"DOW\"]#,\"WEEK\"]#,\"DOY\"]\n",
        "forecast3 = pd.get_dummies(data=forecast2, columns=l3, drop_first=False)\n",
        "\n",
        "### Create Weekend flag\n",
        "#WEEKEND= [0] * len(forecast3.DOW_5)\n",
        "#for i in range(0,len(forecast3.DOW_5)):\n",
        "#    if ((forecast3.DOW_5.iloc[i] == 1) | (forecast3.DOW_6.iloc[i] == 1)): \n",
        "#        WEEKEND[i] = 1 \n",
        "#else: 0\n",
        "\n",
        "#forecast3[\"WEEKEND\"] = pd.Series(WEEKEND).values\n",
        "#tonh3[\"WEEKEND\"] = tonh3[\"WEEKEND\"].astype('category')\n",
        "\nforecast3['repump_1'] = 0"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Predict pre model using forecast\n",
        "x_forecast = ['OAT','TOD_1','TOD_2','TOD_3','TOD_4','TOD_5','TOD_6','TOD_7','TOD_8','TOD_9','TOD_10',\n",
        "              'TOD_11','TOD_12','TOD_13','TOD_14','TOD_15','TOD_16','TOD_17','TOD_18','TOD_19','TOD_20','TOD_21',\n",
        "              'TOD_22','TOD_23','WEEKEND', 'diff1','diff2']\n",
        "\n",
        "forecast_df = pd.DataFrame({\"Date\":forecast3.index,\"Linear Prediction\":lm.predict(forecast3.loc[:,x_forecast])})\n",
        "forecast_df.set_index(forecast_df.Date,inplace=True)\n",
        "\n",
        "ax = forecast_df.plot(figsize=(18,5), linewidth=2, colormap='viridis')\n",
        "ax.set_ylabel('Degrees')\n",
        "ax.set_title('Delta T Forecast')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set new training and test based on all data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "delta_all = delta3.drop(delta3.loc[delta3.diff1.isnull(),:].index, axis=0)\n",
        "X_all = delta_all.ix[int(len(delta_all)-8652):,x]\n",
        "y_all = delta_all.deltat[int(len(delta_all)-8652):]\n",
        "\n",
        "X_all_test = delta_all.ix[0:int(len(delta_all)-8652),x]\n",
        "y_all_test = delta_all.deltat[0:int(len(delta_all)-8652)]\n",
        "\n",
        "X_all_sc = scaler.fit_transform(X_all)\n",
        "X_all_sc_test = scaler.transform(X_all_test)\n",
        "y_all_sc = y_all.as_matrix()\n",
        "y_all_sc_test = y_all_test\n",
        "\n",
        "X_all_sc = np.reshape(X_all_sc, (X_all_sc.shape[0], 1, X_all_sc.shape[1]))\n",
        "X_all_sc_test = np.reshape(X_all_sc_test, (X_all_sc_test.shape[0], 1, X_all_sc_test.shape[1]))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit new model to 'history_all'"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "history_all = model.fit(X_all_sc, y_all_sc, epochs=50, validation_split=0.33,\n",
        "                    batch_size=16, verbose=2)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "\n",
        "# summarize history for loss\n",
        "plt.figure(figsize=(18,5))\n",
        "plt.ylim(1,2)\n",
        "plt.plot(history_all.history['loss'], linewidth=2)\n",
        "plt.plot(history_all.history['val_loss'], linewidth=2)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "y_pred = history_all.model.predict(X_all_sc_test, verbose=0)\n",
        "\n",
        "plt.figure(figsize=(18,5))\n",
        "plt.plot(y_all_sc,linewidth=2)\n",
        "plt.plot(history_all.model.predict(X_all_sc, verbose=0),linewidth=1)\n",
        "plt.legend(['Actual','Predicted'])\n",
        "\n",
        "#plt.figure(figsize=(18,5))\n",
        "#plt.plot(y_all_sc_test[0:30],linewidth=2)\n",
        "#plt.plot(history_all.model.predict(X_all_sc_test, verbose=0)[0:30],linewidth=1)\n",
        "#plt.legend(['Actual','Predicted'])\n",
        "\n",
        "print('R-Squared Train: %f' %(r2_score(y_all_sc, history_all.model.predict(X_all_sc, verbose=0))))\n",
        "print('R-Squared Test: %f' %(r2_score(y_all_sc_test, y_pred)))\n",
        "print(\"Keras RMSE = %.3f\" %(math.sqrt(mean_squared_error(y_all_sc_test, history_all.model.predict(X_all_sc_test, verbose=0)))))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forecast Keras"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_sc_3d = np.reshape(pd.DataFrame(forecast_sc).as_matrix(), (forecast_sc.shape[0],1,forecast_sc.shape[1]))\n",
        "forecast_sc_3d.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "forecast_sc_3d2 = forecast_sc_3d.copy()\n",
        "forecast_sc_3d2 = pd.DataFrame(forecast_sc_3d2.reshape(forecast_sc_3d2.shape[0],forecast_sc_3d2.shape[2]),\n",
        "                               columns=[\"OAT\",\"diff1\"])\n",
        "forecast_unsc = pd.DataFrame(scaler.inverse_transform(forecast_sc_3d2), columns=[\"OAT\",\"diff1\"])\n",
        "for i in range(len(forecast_sc)):\n",
        "    forecast_sc_3d2 = pd.DataFrame(forecast_sc_3d2, columns=[\"OAT\",\"diff1\"])\n",
        "    forecast_sc_3d2 = np.reshape(forecast_sc_3d2.as_matrix(), (forecast_sc_3d2.shape[0],1,forecast_sc_3d2.shape[1]))\n",
        "    \n",
        "    result = history2.model.predict(forecast_sc_3d2)[0:i+1]\n",
        "    \n",
        "    forecast_sc_3d2 = pd.DataFrame(forecast_sc_3d2.reshape(forecast_sc_3d2.shape[0],forecast_sc_3d2.shape[2]),\n",
        "                               columns=[\"OAT\",\"diff1\"])\n",
        "    forecast_unsc = pd.DataFrame(scaler.inverse_transform(forecast_sc_3d2), columns=[\"OAT\",\"diff1\"], index=forecast.index)\n",
        "    #result[i+1] = np.repeat(result[i], (len(result)-i))\n",
        "    forecast_unsc.iloc[:i+1,1] = result.ravel()\n",
        "    forecast_sc_3d2 = scaler.transform(forecast_unsc)\n",
        "    #forecast_sc.iloc[i+1,1] = (result[i] - X_train['diff1'].min(axis=0)) / (X_train['diff1'].max(axis=0) - X_train['diff1'].min(axis=0))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "forecast_unsc.diff1.plot(figsize=(18,5),legend=True)\n",
        "delta3.loc[forecast.index,'deltat'].plot(legend=True, colormap='viridis', title=\"Prediction vs Forecast\")\n",
        "plt.legend([\"Modeled\",\"Actual\"])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18,5))\n",
        "plt.plot(y_test.values[0:100],linewidth=5, color='black', linestyle='--')\n",
        "plt.plot(history2.model.predict(X_test_sc_3d)[0:100],linewidth=3, color='orange', alpha=0.5)\n",
        "plt.plot(sm_fit.predict(X_test_sc)[0:100], linewidth=3, color='green', alpha=0.5)\n",
        "#plt.plot(gs_svr.best_estimator_.predict(X_test_std)[0:50], linewidth=3, color='purple', alpha=0.5)\n",
        "#plt.plot(ann_mod.predict(X_test_std)[0:100], linewidth=3, color='red', alpha=0.5)\n",
        "#plt.plot(y_pred[0:100],linewidth=3, color='orange', alpha=0.5)\n",
        "plt.title(\"Comparison of first 50 values of test period\")\n",
        "#plt.legend(['Actual','Linear','SVR','NN','Keras'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print \"Linear MSE = %.3f\" %lm_rmse**2\n",
        "#print \"Decision Tree RMSE = %.3f\" %(math.sqrt(mean_squared_error(y_test, gs_dt.best_estimator_.predict(X_test))))\n",
        "#print \"Random Forest RMSE = %.3f\" %(math.sqrt(mean_squared_error(y_test, gs_rf.best_estimator_.predict(X_test))))\n",
        "#print \"Boosting RMSE = %.3f\" %(math.sqrt(mean_squared_error(y_test, gs_boost.best_estimator_.predict(X_test))))\n",
        "print \"Support Vector Regression MSE = %.3f\" %(mean_squared_error(y_test, gs_svr.best_estimator_.predict(X_test_std)))\n",
        "print \"Artificial Neural Network MSE = %.3f\" %(mean_squared_error(y_test, ann_mod.predict(X_test_std)))\n",
        "print \"Keras MSE = %.3f\" %(mean_squared_error(y_test, model.predict(X_test_sc)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Models"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python2",
      "language": "python",
      "display_name": "Python 2"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.14",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    },
    "kernel_info": {
      "name": "python2"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}